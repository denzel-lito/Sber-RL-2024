{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "08b030de",
      "metadata": {
        "id": "08b030de"
      },
      "source": [
        "## Метод Actor-Critic"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb3a82f3",
      "metadata": {
        "id": "cb3a82f3"
      },
      "source": [
        "Теорема о градиенте стратегии связывает градиент целевой функции  и градиент самой стратегии:\n",
        "\n",
        "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_\\pi [Q^\\pi(s, a) \\nabla_\\theta \\ln \\pi_\\theta(a \\vert s)]$$\n",
        "\n",
        "Встает вопрос, как оценить $Q^\\pi(s, a)$? Ранее в REINFORCE мы использовали отдачу $R_t$ (полученную методом Монте-Карло) в качестве несмещенной оценки $Q^\\pi(s, a)$. В Actor-Critic же предлагается отдельно обучать нейронную сеть Q-функции - критика.\n",
        "\n",
        "Актор-критиком часто называют обобщенный фреймворк (подход), нежели какой-то конкретный алгоритм. Как подход актор-критик не указывает, каким конкретно [policy gradient] методом обучается актор и каким [value based] методом обучается критик. Таким образом актор-критик задает целое семейство различных алгоритмов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a870514-f49e-4a18-9234-ba682ce1ee07",
      "metadata": {
        "id": "4a870514-f49e-4a18-9234-ba682ce1ee07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d996415-d11d-421d-8d3f-600510e07535"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.5/67.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    pass\n",
        "\n",
        "if COLAB:\n",
        "    !pip -q install \"gymnasium[classic-control, atari, accept-rom-license]\"\n",
        "    !pip -q install piglet\n",
        "    !pip -q install imageio_ffmpeg\n",
        "    !pip -q install moviepy==1.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b38ba055",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "b38ba055",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3fc5161-e0ff-4187-f0e2-903e3e855a54"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb481bf0",
      "metadata": {
        "id": "bb481bf0"
      },
      "source": [
        "### Основной цикл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cedde6d",
      "metadata": {
        "id": "4cedde6d"
      },
      "outputs": [],
      "source": [
        "def print_mean_reward(step, episode_rewards):\n",
        "    if not episode_rewards:\n",
        "        return\n",
        "\n",
        "    t = min(50, len(episode_rewards))\n",
        "    mean_reward = sum(episode_rewards[-t:]) / t\n",
        "    print(f\"step: {str(step).zfill(6)}, mean reward: {mean_reward:.2f}\")\n",
        "    return mean_reward\n",
        "\n",
        "\n",
        "def to_tensor(x, dtype=np.float32):\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return x\n",
        "    if isinstance(x, list) and isinstance(x[0], torch.Tensor):\n",
        "        return torch.asarray(x).to(device)\n",
        "\n",
        "    x = np.asarray(x, dtype=dtype)\n",
        "    x = torch.from_numpy(x).to(device)\n",
        "    return x\n",
        "\n",
        "\n",
        "def run(\n",
        "        env: gym.Env, hidden_size: int, actor_lr: float, critic_lr: float, gamma: float, max_episodes: int,\n",
        "        rollout_size: int, replay_buffer_size: int, critic_batch_size: int, critic_updates_per_actor: int\n",
        "):\n",
        "    # Инициализируем агента\n",
        "    states_dim = env.observation_space.shape[0]\n",
        "    actions_dim = env.action_space.n\n",
        "    agent = ActorCriticAgent(states_dim, actions_dim, hidden_size, actor_lr, critic_lr, gamma, replay_buffer_size)\n",
        "\n",
        "    step = 0\n",
        "    episode_rewards = []\n",
        "\n",
        "    for i_episode in range(1, max_episodes + 1):\n",
        "        cumulative_reward = 0\n",
        "        done = False\n",
        "        state, _ = env.reset()\n",
        "\n",
        "        while not done:\n",
        "            step += 1\n",
        "\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.append_to_replay_buffer(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            cumulative_reward += reward\n",
        "\n",
        "        episode_rewards.append(cumulative_reward)\n",
        "\n",
        "        # выполняем обновление\n",
        "        if agent.update(rollout_size, critic_batch_size, critic_updates_per_actor):\n",
        "            mean_reward = print_mean_reward(step, episode_rewards)\n",
        "            if mean_reward >= 200:\n",
        "                print('Принято!')\n",
        "                return\n",
        "            episode_rewards = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14d624df",
      "metadata": {
        "id": "14d624df"
      },
      "outputs": [],
      "source": [
        "from collections import deque, namedtuple\n",
        "\n",
        "class ActorBatch:\n",
        "    def __init__(self):\n",
        "        self.logprobs = []\n",
        "        self.q_values = []\n",
        "\n",
        "    def append(self, log_prob, q_value):\n",
        "        self.logprobs.append(log_prob)\n",
        "        self.q_values.append(q_value)\n",
        "\n",
        "    def clear(self):\n",
        "        self.logprobs.clear()\n",
        "        self.q_values.clear()\n",
        "\n",
        "\n",
        "Transition = namedtuple('Transition', ['state', 'action', 'reward', 'next_state', 'done'])\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, size):\n",
        "        self.buffer = deque(maxlen=size)\n",
        "\n",
        "    def append(self, state, action, reward, next_state, done):\n",
        "        sample = Transition(state, action, reward, next_state, done)\n",
        "        self.buffer.append(sample)\n",
        "\n",
        "    def sample_batch(self, n_samples):\n",
        "        indices = np.random.choice(len(self.buffer), n_samples)\n",
        "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "        for i in indices:\n",
        "            s, a, r, n_s, done = self.buffer[i]\n",
        "            states.append(s)\n",
        "            actions.append(a)\n",
        "            rewards.append(r)\n",
        "            next_states.append(n_s)\n",
        "            dones.append(done)\n",
        "\n",
        "        batch = np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62b2641d",
      "metadata": {
        "id": "62b2641d"
      },
      "outputs": [],
      "source": [
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        state = to_tensor(state)\n",
        "        return self.net(state)\n",
        "\n",
        "\n",
        "class ActorCriticModel(nn.Module):\n",
        "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Инициализируем сеть агента с двумя головами: softmax-актора и линейного критика\n",
        "        self.net = MLPModel(state_dim, action_dim, hidden_dim).to(device)\n",
        "        self.actor_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        self.critic_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, action_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        hidden_state = self.net(state)\n",
        "        # Вычислите выбранное действие, логарифм вероятности его выбора и соответствующее значение Q-функции\n",
        "        ####### Здесь ваш код ########\n",
        "        action_probs = self.actor_head(hidden_state)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "\n",
        "        log_prob = dist.log_prob(action)\n",
        "        action = action.item()\n",
        "\n",
        "        q_value = self.critic_head(hidden_state)[action]\n",
        "        ##############################\n",
        "\n",
        "        return action, log_prob, q_value\n",
        "\n",
        "    def evaluate(self, state):\n",
        "        # Вычислите значения Q-функции для данного состояния\n",
        "        ####### Здесь ваш код ########\n",
        "        hidden_state = self.net(state)\n",
        "        q_values = self.critic_head(hidden_state)\n",
        "        ##############################\n",
        "        return q_values\n",
        "\n",
        "\n",
        "class ActorCriticAgent:\n",
        "    def __init__(self, state_dim, action_dim, hidden_size, actor_lr, critic_lr, gamma, replay_buffer_size):\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Инициализируем модель актор-критика и SGD оптимизатор\n",
        "        self.actor_critic = ActorCriticModel(state_dim, hidden_size, action_dim).to(device)\n",
        "        self.opt_actor = torch.optim.Adam(self.actor_critic.parameters(), lr=actor_lr)\n",
        "        self.opt_critic = torch.optim.Adam(self.actor_critic.parameters(), lr=critic_lr)\n",
        "\n",
        "        self.actor_batch = ActorBatch()\n",
        "        self.replay_buffer = ReplayBuffer(replay_buffer_size)\n",
        "\n",
        "    def act(self, state):\n",
        "        # Произведите выбор действия и сохраните необходимые данные в батч для последующего обучения\n",
        "        # Не забудьте сделать q_value.detach()\n",
        "        # self.actor_batch.append(..)\n",
        "        ####### Здесь ваш код ########\n",
        "        action, logprob, q_value = self.actor_critic(state)\n",
        "        self.actor_batch.append(logprob, q_value.detach())\n",
        "        ##############################\n",
        "\n",
        "        return action\n",
        "\n",
        "    def evaluate(self, state):\n",
        "        return self.actor_critic.evaluate(state)\n",
        "\n",
        "    def update(self, rollout_size, critic_batch_size, critic_updates_per_actor):\n",
        "        if len(self.actor_batch.q_values) < rollout_size:\n",
        "            return False\n",
        "\n",
        "        self.update_actor()\n",
        "        self.update_critic(critic_batch_size, critic_updates_per_actor)\n",
        "        self.actor_batch.clear()\n",
        "        return True\n",
        "\n",
        "    def update_actor(self):\n",
        "        q_values = to_tensor(self.actor_batch.q_values)\n",
        "        logprobs = torch.stack(self.actor_batch.logprobs).to(device)\n",
        "\n",
        "        # Реализуйте шаг обновления актора. Опционально: сделайте нормализацию полезностей\n",
        "        ####### Здесь ваш код ########\n",
        "        # Нормализация полезностей\n",
        "        q_values = (q_values - q_values.mean()) / (q_values.std() + 1e-7)\n",
        "\n",
        "        # Вычислите ошибку `loss`\n",
        "        # loss = ...\n",
        "        loss = - logprobs * q_values\n",
        "        loss = loss.mean()\n",
        "        ##############################\n",
        "\n",
        "        self.opt_actor.zero_grad()\n",
        "        loss.backward()\n",
        "        self.opt_actor.step()\n",
        "\n",
        "    def update_critic(self, batch_size, critic_updates_per_actor):\n",
        "        # ограничивает сверху количество эпох для буфера небольшого размера\n",
        "        critic_updates_per_actor = min(\n",
        "            critic_updates_per_actor,\n",
        "            5 * len(self.replay_buffer.buffer) // batch_size\n",
        "        )\n",
        "\n",
        "        for _ in range(critic_updates_per_actor):\n",
        "            train_batch = self.replay_buffer.sample_batch(batch_size)\n",
        "            states, actions, rewards, next_states, is_done = train_batch\n",
        "\n",
        "            self.opt_critic.zero_grad()\n",
        "            loss, _ = self.compute_td_loss(states, actions, rewards, next_states, is_done, regularizer=0.01)\n",
        "            loss.backward()\n",
        "            self.opt_critic.step()\n",
        "\n",
        "    def append_to_replay_buffer(self, s, a, r, next_s, done):\n",
        "        # Добавьте новый экземпляр данных в память прецедентов.\n",
        "        self.replay_buffer.append(s, a, r, next_s, done)\n",
        "\n",
        "    def compute_td_loss(\n",
        "        self, states, actions, rewards, next_states, is_done, check_shapes=False, regularizer=0.0\n",
        "    ):\n",
        "        \"\"\" Считатет td ошибку, используя лишь операции фреймворка torch\"\"\"\n",
        "\n",
        "        # переводим входные данные в тензоры\n",
        "        states = to_tensor(states)                      # shape: [batch_size, state_size]\n",
        "        actions = to_tensor(actions, int).long()        # shape: [batch_size]\n",
        "        rewards = to_tensor(rewards)                    # shape: [batch_size]\n",
        "        next_states = to_tensor(next_states)            # shape: [batch_size, state_size]\n",
        "        is_done = to_tensor(is_done, bool)              # shape: [batch_size]\n",
        "\n",
        "        # получаем значения q для всех действий из текущих состояний\n",
        "        predicted_qvalues = self.evaluate(states)\n",
        "\n",
        "        # получаем q-values для выбранных действий\n",
        "        predicted_qvalues_for_actions = predicted_qvalues[range(states.shape[0]), actions]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predicted_next_qvalues = self.evaluate(next_states)\n",
        "            next_state_values = torch.max(predicted_next_qvalues, axis=-1)[0]\n",
        "\n",
        "            assert next_state_values.dtype == torch.float32\n",
        "\n",
        "            # вычисляем target q-values для функции потерь\n",
        "            #  target_qvalues_for_actions =\n",
        "            target_qvalues_for_actions = rewards + self.gamma * next_state_values\n",
        "\n",
        "            # для последнего действия в эпизоде используем\n",
        "            # упрощенную формулу Q(s,a) = r(s,a),\n",
        "            # т.к. s' для него не существует\n",
        "            target_qvalues_for_actions = torch.where(is_done, rewards, target_qvalues_for_actions)\n",
        "\n",
        "        losses = (predicted_qvalues_for_actions - target_qvalues_for_actions) ** 2\n",
        "\n",
        "        # MSE loss для минимизации\n",
        "        loss = torch.mean(losses)\n",
        "        # добавляем регуляризацию на значения Q\n",
        "        loss += regularizer * predicted_qvalues_for_actions.mean()\n",
        "\n",
        "        return loss, losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3366c97f",
      "metadata": {
        "id": "3366c97f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "223371f7-39f9-4328-9a60-5b6035df3a8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 001004, mean reward: 20.92\n",
            "step: 002009, mean reward: 11.70\n",
            "step: 003017, mean reward: 9.82\n",
            "step: 004019, mean reward: 9.50\n",
            "step: 005022, mean reward: 9.54\n",
            "step: 006028, mean reward: 9.48\n",
            "step: 007038, mean reward: 12.26\n",
            "step: 008048, mean reward: 9.62\n",
            "step: 009058, mean reward: 24.63\n",
            "step: 010061, mean reward: 22.80\n",
            "step: 011068, mean reward: 21.89\n",
            "step: 012074, mean reward: 17.84\n",
            "step: 013097, mean reward: 30.09\n",
            "step: 014107, mean reward: 11.90\n",
            "step: 015125, mean reward: 33.93\n",
            "step: 016131, mean reward: 19.62\n",
            "step: 017140, mean reward: 33.63\n",
            "step: 018150, mean reward: 20.20\n",
            "step: 019188, mean reward: 45.13\n",
            "step: 020202, mean reward: 28.17\n",
            "step: 021242, mean reward: 65.00\n",
            "step: 022256, mean reward: 44.09\n",
            "step: 023310, mean reward: 65.88\n",
            "step: 024330, mean reward: 72.86\n",
            "step: 025342, mean reward: 63.25\n",
            "step: 026372, mean reward: 73.57\n",
            "step: 027413, mean reward: 74.36\n",
            "step: 028423, mean reward: 72.14\n",
            "step: 029487, mean reward: 76.00\n",
            "step: 030532, mean reward: 80.38\n",
            "step: 031618, mean reward: 83.54\n",
            "step: 032699, mean reward: 90.08\n",
            "step: 033760, mean reward: 96.45\n",
            "step: 034815, mean reward: 105.50\n",
            "step: 035859, mean reward: 116.00\n",
            "step: 036871, mean reward: 126.50\n",
            "step: 037975, mean reward: 138.00\n",
            "step: 039064, mean reward: 136.12\n",
            "step: 040176, mean reward: 139.00\n",
            "step: 041176, mean reward: 142.86\n",
            "step: 042237, mean reward: 151.57\n",
            "step: 043369, mean reward: 161.71\n",
            "step: 044408, mean reward: 207.80\n",
            "Принято!\n"
          ]
        }
      ],
      "source": [
        "from gymnasium.wrappers.time_limit import TimeLimit\n",
        "env_name = \"CartPole-v1\"\n",
        "\n",
        "run(\n",
        "    env = TimeLimit(gym.make(env_name), 500),\n",
        "    max_episodes = 50000,  # количество эпизодов обучения\n",
        "    hidden_size = 64,  # кол-во переменных в скрытых слоях\n",
        "    rollout_size = 1000,  # через столько шагов стратегия будет обновляться\n",
        "    actor_lr = 0.01,\n",
        "    critic_lr = 0.01,\n",
        "    gamma = 0.99,  # дисконтирующий множитель,\n",
        "    replay_buffer_size = 20000,\n",
        "    critic_batch_size = 512,\n",
        "    critic_updates_per_actor = 32,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4977dd5e",
      "metadata": {
        "id": "4977dd5e"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mipt-rl-practice",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16 (main, Mar  8 2023, 14:00:05) \n[GCC 11.2.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "f4f1d2d8cbda689bee08cb8d7fe5a19f770d75378302676f04af068af2c2973a"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}