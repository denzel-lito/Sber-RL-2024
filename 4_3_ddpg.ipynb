{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cjehRbPEVRh"
      },
      "source": [
        "# Deep Deterministic Policy Gradient\n",
        "\n",
        "На этом семинаре мы будем обучать нейронную сеть на фреймворке __pytorch__ с помощью алгоритма Deep Deterministic Policy Gradient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYHGD-j9EVRi"
      },
      "source": [
        "## Теория\n",
        "\n",
        "Deep Deterministic Policy Gradient (DDPG) - это алгоритм, который одновременно учит Q-функцию и стратегию. Он использует off-policy данные и уравнения Беллмана для обучения Q-функции, а Q-функция используется для обучения стратегии.\n",
        "\n",
        "Данный подход тесно связан с Q-обучением и мотивирован следующей идеей: если вы знаете оптимальную функцию action-value $Q^*(s,a)$, тогда для конкретного состояния, оптимальное действие $a^*(s)$ может быть найдено решением:\n",
        "\n",
        "$$a^*(s) = \\arg \\max_a Q^*(s,a).$$\n",
        "\n",
        "Для сред с дискретным пространством действий - это легко, вычисляем полезности для каждого из действий, а потом берем максимум. Для непрерывных действий - это сложная оптимизационная задача.\n",
        "\n",
        "DDPG чередует обучение аппроксиматора $Q^*(s,a)$ с обучением аппроксиматора  $a^*(s)$, и делает это специальным образом именно для непрерывных (continuous) сред, что отражается в том как алгоритм вычисляет $\\max_a Q^*(s,a)$.\n",
        "Поскольку пространство действий непрерывно, предполагается, что функция $Q^*(s,a)$ дифференцируема по аргументу действия. Это позволяет нам установить эффективное правило обучения на основе градиента для стратегии $\\mu(s)$.\n",
        "\n",
        "<img src=\"https://spinningup.openai.com/en/latest/_images/math/5811066e89799e65be299ec407846103fcf1f746.svg\">\n",
        "\n",
        "Оригинальная статья:  <a href=\"https://arxiv.org/abs/1509.02971\">Continuous control with deep reinforcement learning Arxiv</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvDl146Zm26M"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    pass\n",
        "\n",
        "if COLAB:\n",
        "    !pip -q install \"gymnasium[classic-control, atari, accept-rom-license]\"\n",
        "    !pip -q install piglet\n",
        "    !pip -q install imageio_ffmpeg\n",
        "    !pip -q install moviepy==1.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "wWhT_K8Ym26P"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijfd7VKnm26Q"
      },
      "source": [
        "### Вспомогательные функции"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ktt4D4nzm26R"
      },
      "outputs": [],
      "source": [
        "def print_mean_reward(step, session_rewards, eval_session_rewards = None):\n",
        "    if not session_rewards:\n",
        "        return\n",
        "\n",
        "    def get_mean_reward(rewards):\n",
        "        return round(sum(rewards) / len(rewards), 2)\n",
        "\n",
        "    train_mean = get_mean_reward(session_rewards)\n",
        "    eval_mean = None\n",
        "    if eval_session_rewards is not None:\n",
        "        eval_mean = get_mean_reward(eval_session_rewards)\n",
        "\n",
        "    print(f\"step: {str(step).zfill(6)}, train: {train_mean}, eval: {eval_mean}\")\n",
        "    return train_mean if eval_mean is None else eval_mean\n",
        "\n",
        "\n",
        "def to_tensor(x, dtype=np.float32):\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return x\n",
        "    x = np.asarray(x, dtype=dtype)\n",
        "    x = torch.from_numpy(x).to(device)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqgijaO0m26S"
      },
      "source": [
        "## Batch/replay buffer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "iaGdvgeLm26T"
      },
      "outputs": [],
      "source": [
        "from collections import deque, namedtuple\n",
        "\n",
        "Transition = namedtuple('Transition', ['state', 'action', 'reward', 'next_state', 'done'])\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, size):\n",
        "        self.buffer = deque(maxlen=size)\n",
        "        self.rng = np.random.default_rng()\n",
        "\n",
        "    def append(self, state, action, reward, next_state, done):\n",
        "        sample = Transition(state, action, reward, next_state, done)\n",
        "        self.buffer.append(sample)\n",
        "\n",
        "    def sample_batch(self, batch_size):\n",
        "        indices = self.rng.choice(len(self.buffer), batch_size, replace=False)\n",
        "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "        for i in indices:\n",
        "            s, a, r, n_s, done = self.buffer[i]\n",
        "            states.append(s)\n",
        "            actions.append(a)\n",
        "            rewards.append(r)\n",
        "            next_states.append(n_s)\n",
        "            dones.append(done)\n",
        "\n",
        "        batch = np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
        "        return batch, indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vlzBWF6EVRk"
      },
      "source": [
        "## Исследование - GaussNoise\n",
        "Добавляем Гауссовский шум к действиям детерминированной стратегии.\n",
        "Добавляем его только при обучении для исследования.\n",
        "\n",
        "NB: вы также можете погуглить реализации шума из распределения Орнштейна-Уленбека (Ornstein-Uhlenbeck) — это даст агенту более качественное направленное исследование."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JZ9_FRdEVRk"
      },
      "outputs": [],
      "source": [
        "class GaussNoise:\n",
        "    def __init__(self, sigma):\n",
        "        super().__init__()\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def sample(self, action):\n",
        "        return np.random.normal(action, self.sigma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5qA-jz-EVRk"
      },
      "source": [
        "## DDPG Network\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/Tviskaron/mipt/master/2020/RL/figures/DQN-DDPG.svg\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qevVTJLVm26Z"
      },
      "source": [
        "### DDPG Model\n",
        "Реализуйте модель актор-критика `DdpgModel`. Можете реализовать актор-критика единым модулем, а можете разнести их (первый вариант не обязательно предполагает общее тело сетей). `DdpgModel` также не обязательно делать наследником `nn.Module`.\n",
        "\n",
        "NB: часто рекомендуется инициализировать последний слой актора весами с небольшими по модулю значениями ($\\sim 10^{-3}$)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ibh3quxem26Z"
      },
      "outputs": [],
      "source": [
        "class DdpgModel:\n",
        "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self._policy = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, action_dim),\n",
        "            nn.Tanh(),\n",
        "        ).to(device)\n",
        "\n",
        "        init_w = 1e-3\n",
        "        self._policy[-2].weight.data.uniform_(-init_w, init_w)\n",
        "\n",
        "        self._q = nn.Sequential(\n",
        "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        ).to(device)\n",
        "\n",
        "    def act(self, s):\n",
        "        s = to_tensor(s)\n",
        "        ####### Здесь ваш код ########\n",
        "        return ...\n",
        "        ##############################\n",
        "\n",
        "    def Q(self, s, a):\n",
        "        s, a = to_tensor(s), to_tensor(a)\n",
        "        sa = torch.cat([s, a], 1)\n",
        "        ####### Здесь ваш код ########\n",
        "        return ...\n",
        "        ##############################\n",
        "\n",
        "    def V(self, s):\n",
        "        s = to_tensor(s)\n",
        "        ####### Здесь ваш код ########\n",
        "        return ...\n",
        "        ##############################\n",
        "\n",
        "    def copy_params_to(self, model: 'DdpgModel', lr=1.0):\n",
        "        self.alpha_merge_params(target=model._policy, other=self._policy, lr=lr)\n",
        "        self.alpha_merge_params(target=model._q, other=self._q, lr=lr)\n",
        "\n",
        "    @staticmethod\n",
        "    def alpha_merge_params(target: nn.Module, other: nn.Module, lr):\n",
        "        state_dict, other_dict = target.state_dict(), other.state_dict()\n",
        "        target.load_state_dict({\n",
        "            key: (1 - lr) * state_dict[key] + lr * other_dict[key]\n",
        "            for key in state_dict\n",
        "        })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9agVxv8Vm26a"
      },
      "source": [
        "### DDPG Agent\n",
        "\n",
        "Реализуйте класс агента `DdpgAgent`, который содержит:\n",
        "\n",
        "- обучаемую модель актор-критика и его оптимизаторы (например, `torch.optim.Adam`)\n",
        "- периодически обновляемую копию модели с замороженными весами для вычисления TD target.\n",
        "- метод `act` для выбора действия. Можете добавить флаг `learn` для подмешивания гауссовского шума при обучении, либо добавляйте его снаружи агента.\n",
        "- метод `learn` для обновления модели агента (и актора, и критика) на новом пакете опыта."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX0ZNJYnm26b"
      },
      "outputs": [],
      "source": [
        "class DdpgAgent:\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim, lr, gamma, soft_tau, noise_sigma, replay_buffer_size):\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.soft_tau = soft_tau\n",
        "        self.noise = GaussNoise(sigma=noise_sigma)\n",
        "\n",
        "        # Инициализируйте модель актор-критика и SGD оптимизатор (например, `torch.optim.Adam)`)\n",
        "        self.model = DdpgModel(state_dim, hidden_dim, action_dim)\n",
        "        self.actor_optim = torch.optim.Adam(self.model._policy.parameters(), lr=0.3 * lr)\n",
        "        self.critic_optim = torch.optim.Adam(self.model._q.parameters(), lr=lr)\n",
        "\n",
        "        self.target_model = DdpgModel(state_dim, hidden_dim, action_dim)\n",
        "        self.model.copy_params_to(self.target_model)\n",
        "\n",
        "        self.replay_buffer = ReplayBuffer(replay_buffer_size)\n",
        "\n",
        "    def act(self, state, *, learn):\n",
        "        with torch.no_grad():\n",
        "            action = self.model.act(state).cpu().numpy()\n",
        "\n",
        "        if learn:\n",
        "            action = self.noise.sample(action)\n",
        "        return action\n",
        "\n",
        "    def update(self, batch_size, sgd_steps_per_update):\n",
        "        if len(self.replay_buffer) < batch_size:\n",
        "            return\n",
        "\n",
        "        # ограничивает сверху количество эпох для буфера небольшого размера\n",
        "        sgd_steps_per_update = min(sgd_steps_per_update, 2 * len(self.replay_buffer) // batch_size)\n",
        "        for _ in range(sgd_steps_per_update):\n",
        "            train_batch, indices = self.replay_buffer.sample_batch(batch_size)\n",
        "            states, actions, rewards, next_states, is_done = train_batch\n",
        "\n",
        "            states = to_tensor(states)                             # shape: [batch_size, state_size]\n",
        "            actions = to_tensor(actions)                           # shape: [batch_size, 1]\n",
        "            rewards = to_tensor(rewards).unsqueeze(1)              # shape: [batch_size, 1]\n",
        "            next_states = to_tensor(next_states)                   # shape: [batch_size, state_size]\n",
        "            is_done = to_tensor(is_done, bool).unsqueeze(1)        # shape: [batch_size, 1]\n",
        "\n",
        "            self.update_actor(states)\n",
        "            self.update_critic(states, actions, rewards, next_states, is_done)\n",
        "\n",
        "        # update target model\n",
        "        self.model.copy_params_to(self.target_model, lr=self.soft_tau)\n",
        "\n",
        "    def update_actor(self, states):\n",
        "        # loss = ...\n",
        "        ####### Здесь ваш код ########\n",
        "        loss = ...\n",
        "        ##############################\n",
        "        self.actor_optim.zero_grad()\n",
        "        loss.backward()\n",
        "        self.actor_optim.step()\n",
        "\n",
        "    def update_critic(self, states, actions, rewards, next_states, is_done):\n",
        "        q_values = self.model.Q(states, actions)\n",
        "        with torch.no_grad():\n",
        "            V_next = self.target_model.V(next_states)\n",
        "            td_target = rewards + self.gamma * torch.logical_not(is_done) * V_next\n",
        "\n",
        "        # loss = ...\n",
        "        ####### Здесь ваш код ########\n",
        "        loss = ...\n",
        "        ##############################\n",
        "\n",
        "        self.critic_optim.zero_grad()\n",
        "        loss.backward()\n",
        "        self.critic_optim.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z8uH7Gnm26b"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Реализуйте функцию `run`, которая принимает среду, гиперпараметры агента и условие останова эксперимента (return threshold $G_{target}$). Используйте функцию `print_mean_reward` для вывода промежуточных результатов качества агента в трейн и eval режимах.\n",
        "\n",
        "Проведите эксперимент на среде с непрерывным пространством действий (например, continuous montain car или pendulum)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXGMKmFbm26c"
      },
      "outputs": [],
      "source": [
        "from gymnasium.experimental.wrappers import RescaleActionV0\n",
        "\n",
        "def run_episode(\n",
        "        env: gym.Env, agent: DdpgAgent, step: int, train: bool,\n",
        "        steps_per_update: int, batch_size: int, update_iterations: int\n",
        "):\n",
        "    episode_reward = 0\n",
        "    done = False\n",
        "    state, _ = env.reset()\n",
        "\n",
        "    while not done:\n",
        "        step += 1\n",
        "\n",
        "        action = agent.act(state, learn=train)\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        if train:\n",
        "            agent.replay_buffer.append(state, action, reward, next_state, done)\n",
        "            if step % steps_per_update == 0:\n",
        "                agent.update(batch_size, update_iterations)\n",
        "\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "\n",
        "    return step, episode_reward\n",
        "\n",
        "\n",
        "def run(\n",
        "        env: gym.Env, hidden_size: int, lr: float, gamma: float, soft_tau: float, max_episodes: int,\n",
        "        noise_sigma: float, replay_buffer_size: int,\n",
        "        steps_per_update: int, batch_size: int, update_iterations: int,\n",
        "        success_reward: float\n",
        "):\n",
        "    env = RescaleActionV0(env, min_action=-1.0, max_action=1.0)\n",
        "\n",
        "    states_dim = env.observation_space.shape[0]\n",
        "    actions_dim = env.action_space.shape[0]\n",
        "    agent = DdpgAgent(states_dim, actions_dim, hidden_size, lr, gamma, soft_tau, noise_sigma, replay_buffer_size)\n",
        "\n",
        "    step = 0\n",
        "    train_results, eval_results = [], []\n",
        "    for i_episode in range(1, max_episodes + 1):\n",
        "        _, episode_reward = run_episode(\n",
        "            env, agent, step, train=False,\n",
        "            steps_per_update=steps_per_update, batch_size=batch_size, update_iterations=update_iterations\n",
        "        )\n",
        "        eval_results.append(episode_reward)\n",
        "\n",
        "        step, episode_reward = run_episode(\n",
        "            env, agent, step, train=True,\n",
        "            steps_per_update=steps_per_update, batch_size=batch_size, update_iterations=update_iterations\n",
        "        )\n",
        "        train_results.append(episode_reward)\n",
        "\n",
        "        if i_episode % 5 == 0 and print_mean_reward(step, train_results[-10:], eval_results[-10:]) >= success_reward:\n",
        "            print('Принято!')\n",
        "            return\n",
        "\n",
        "# env, success = gym.make(\"MountainCarContinuous-v0\", max_episode_steps=1000), 95.0\n",
        "env, success = gym.make(\"Pendulum-v1\", max_episode_steps=200), -200\n",
        "\n",
        "# run experiment\n",
        "run(\n",
        "    env = env,\n",
        "    max_episodes = 100,  # количество эпизодов обучения\n",
        "    hidden_size = 128,  # кол-во переменных в скрытых слоях\n",
        "    lr = 0.001, # learning rate\n",
        "    gamma = 0.995,  # дисконтирующий множитель,\n",
        "    soft_tau = 0.01, # скорость обновления target сети\n",
        "    replay_buffer_size = 5000,\n",
        "    noise_sigma = 0.15,\n",
        "    steps_per_update = 10,  # через столько шагов стратегия будет обновляться\n",
        "    batch_size = 100,\n",
        "    update_iterations = 4,\n",
        "    success_reward=success\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LhCxROE3qH8J"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mipt-rl-practice",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16 (main, Mar  8 2023, 14:00:05) \n[GCC 11.2.0]"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "vscode": {
      "interpreter": {
        "hash": "f4f1d2d8cbda689bee08cb8d7fe5a19f770d75378302676f04af068af2c2973a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}