{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/16971921/Documents/my_python_work/home_venv/lib/python3.6/site-packages/gymnasium/core.py:27: UserWarning: \u001B[33mWARN: Gymnasium minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\u001B[0m\n",
      "  \"Gymnasium minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "def initialize_q_table(observation_space_n, action_space_n):\n",
    "    Q = np.zeros([observation_space_n, action_space_n])\n",
    "    return Q\n",
    "\n",
    "\n",
    "def select_action_eps_greedy(Q, state, epsilon):\n",
    "    # TODO\n",
    "    if np.random.rand() < epsilon:\n",
    "        action = np.random.randint(len(Q[state]))\n",
    "    else:\n",
    "        action = np.argmax(Q[state])\n",
    "    return action\n",
    "\n",
    "\n",
    "def update_Q_SARSA(Q, s, a, r, next_s, next_a, alpha, gamma):\n",
    "    # TODO\n",
    "    target = r + gamma * Q[next_s, next_a]\n",
    "    Q[s, a] += alpha * (target - Q[s,a])\n",
    "\n",
    "\n",
    "def update_Q(Q, s, a, r, next_s, next_a, alpha, gamma):\n",
    "    # TODO\n",
    "    target = r + gamma * np.max(Q[next_s])\n",
    "    Q[s, a] += alpha * (target - Q[s,a])\n",
    "\n",
    "\n",
    "def learn(method):\n",
    "    env = gym.make('CliffWalking-v0')\n",
    "    # определяем память, в которой будет храниться Q(s,a)\n",
    "    Q = initialize_q_table(env.observation_space.n, env.action_space.n)\n",
    "\n",
    "    # гиперпараметры алгоритма (не меняйте параметры)\n",
    "    alpha = 0.1\n",
    "    gamma = 0.9\n",
    "    max_epsilon = 0.2\n",
    "    episodes_number = 10000\n",
    "\n",
    "    for episode in range(1, episodes_number + 1):\n",
    "        epsilon = max_epsilon * (episodes_number - episode) / (episodes_number - 1)\n",
    "        s, _ = env.reset()\n",
    "\n",
    "        r, episode_reward = 0, 0\n",
    "        done = False\n",
    "        a = select_action_eps_greedy(Q, s, epsilon)\n",
    "        while not done:\n",
    "            next_s, r, terminated, truncated, info = env.step(a)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            next_a = select_action_eps_greedy(Q, next_s, epsilon)\n",
    "            ##############################\n",
    "            # Обновите Q функцию в соответствии с алгоритмом SARSA или Q обучение\n",
    "            if method == 'SARSA':\n",
    "                update_Q_SARSA(Q, s, a, r, next_s, next_a, alpha, gamma)\n",
    "            else:\n",
    "                update_Q(Q, s, a, r, next_s, next_a, alpha, gamma)\n",
    "            # Note: считаем Q функцию для терминальных состояний всегда равной 0\n",
    "            ##############################\n",
    "\n",
    "            s = next_s\n",
    "            a = next_a\n",
    "            episode_reward += r\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode: {episode}, Reward: {episode_reward}, Eps: {epsilon}\")\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100, Reward: -139, Eps: 0.19801980198019803\n",
      "Episode: 200, Reward: -25, Eps: 0.19601960196019602\n",
      "Episode: 300, Reward: -17, Eps: 0.19401940194019401\n",
      "Episode: 400, Reward: -222, Eps: 0.192019201920192\n",
      "Episode: 500, Reward: -126, Eps: 0.19001900190019003\n",
      "Episode: 600, Reward: -17, Eps: 0.18801880188018802\n",
      "Episode: 700, Reward: -232, Eps: 0.186018601860186\n",
      "Episode: 800, Reward: -19, Eps: 0.18401840184018403\n",
      "Episode: 900, Reward: -120, Eps: 0.18201820182018202\n",
      "Episode: 1000, Reward: -766, Eps: 0.18001800180018002\n",
      "Episode: 1100, Reward: -341, Eps: 0.178017801780178\n",
      "Episode: 1200, Reward: -15, Eps: 0.17601760176017603\n",
      "Episode: 1300, Reward: -16, Eps: 0.17401740174017402\n",
      "Episode: 1400, Reward: -17, Eps: 0.172017201720172\n",
      "Episode: 1500, Reward: -13, Eps: 0.17001700170017\n",
      "Episode: 1600, Reward: -15, Eps: 0.16801680168016803\n",
      "Episode: 1700, Reward: -17, Eps: 0.16601660166016602\n",
      "Episode: 1800, Reward: -15, Eps: 0.164016401640164\n",
      "Episode: 1900, Reward: -13, Eps: 0.162016201620162\n",
      "Episode: 2000, Reward: -18, Eps: 0.16001600160016002\n",
      "Episode: 2100, Reward: -13, Eps: 0.15801580158015802\n",
      "Episode: 2200, Reward: -15, Eps: 0.156015601560156\n",
      "Episode: 2300, Reward: -13, Eps: 0.15401540154015403\n",
      "Episode: 2400, Reward: -13, Eps: 0.15201520152015202\n",
      "Episode: 2500, Reward: -17, Eps: 0.15001500150015\n",
      "Episode: 2600, Reward: -15, Eps: 0.148014801480148\n",
      "Episode: 2700, Reward: -16, Eps: 0.14601460146014603\n",
      "Episode: 2800, Reward: -17, Eps: 0.14401440144014402\n",
      "Episode: 2900, Reward: -15, Eps: 0.142014201420142\n",
      "Episode: 3000, Reward: -15, Eps: 0.14001400140014\n",
      "Episode: 3100, Reward: -13, Eps: 0.13801380138013802\n",
      "Episode: 3200, Reward: -14, Eps: 0.13601360136013602\n",
      "Episode: 3300, Reward: -117, Eps: 0.134013401340134\n",
      "Episode: 3400, Reward: -17, Eps: 0.132013201320132\n",
      "Episode: 3500, Reward: -454, Eps: 0.13001300130013002\n",
      "Episode: 3600, Reward: -17, Eps: 0.128012801280128\n",
      "Episode: 3700, Reward: -15, Eps: 0.126012601260126\n",
      "Episode: 3800, Reward: -226, Eps: 0.12401240124012401\n",
      "Episode: 3900, Reward: -15, Eps: 0.12201220122012202\n",
      "Episode: 4000, Reward: -120, Eps: 0.12001200120012001\n",
      "Episode: 4100, Reward: -17, Eps: 0.11801180118011802\n",
      "Episode: 4200, Reward: -13, Eps: 0.11601160116011601\n",
      "Episode: 4300, Reward: -13, Eps: 0.11401140114011402\n",
      "Episode: 4400, Reward: -13, Eps: 0.11201120112011201\n",
      "Episode: 4500, Reward: -13, Eps: 0.11001100110011001\n",
      "Episode: 4600, Reward: -13, Eps: 0.10801080108010801\n",
      "Episode: 4700, Reward: -113, Eps: 0.10601060106010601\n",
      "Episode: 4800, Reward: -15, Eps: 0.104010401040104\n",
      "Episode: 4900, Reward: -17, Eps: 0.10201020102010201\n",
      "Episode: 5000, Reward: -15, Eps: 0.1000100010001\n",
      "Episode: 5100, Reward: -13, Eps: 0.09800980098009801\n",
      "Episode: 5200, Reward: -13, Eps: 0.096009600960096\n",
      "Episode: 5300, Reward: -13, Eps: 0.09400940094009401\n",
      "Episode: 5400, Reward: -17, Eps: 0.09200920092009202\n",
      "Episode: 5500, Reward: -122, Eps: 0.09000900090009001\n",
      "Episode: 5600, Reward: -13, Eps: 0.08800880088008801\n",
      "Episode: 5700, Reward: -13, Eps: 0.086008600860086\n",
      "Episode: 5800, Reward: -14, Eps: 0.08400840084008401\n",
      "Episode: 5900, Reward: -15, Eps: 0.082008200820082\n",
      "Episode: 6000, Reward: -13, Eps: 0.08000800080008001\n",
      "Episode: 6100, Reward: -15, Eps: 0.078007800780078\n",
      "Episode: 6200, Reward: -13, Eps: 0.07600760076007601\n",
      "Episode: 6300, Reward: -13, Eps: 0.074007400740074\n",
      "Episode: 6400, Reward: -13, Eps: 0.07200720072007201\n",
      "Episode: 6500, Reward: -119, Eps: 0.07000700070007\n",
      "Episode: 6600, Reward: -13, Eps: 0.06800680068006801\n",
      "Episode: 6700, Reward: -14, Eps: 0.066006600660066\n",
      "Episode: 6800, Reward: -233, Eps: 0.064006400640064\n",
      "Episode: 6900, Reward: -13, Eps: 0.062006200620062006\n",
      "Episode: 7000, Reward: -13, Eps: 0.060006000600060005\n",
      "Episode: 7100, Reward: -13, Eps: 0.058005800580058005\n",
      "Episode: 7200, Reward: -123, Eps: 0.056005600560056004\n",
      "Episode: 7300, Reward: -13, Eps: 0.054005400540054004\n",
      "Episode: 7400, Reward: -118, Eps: 0.052005200520052\n",
      "Episode: 7500, Reward: -13, Eps: 0.05000500050005\n",
      "Episode: 7600, Reward: -13, Eps: 0.048004800480048\n",
      "Episode: 7700, Reward: -13, Eps: 0.04600460046004601\n",
      "Episode: 7800, Reward: -13, Eps: 0.04400440044004401\n",
      "Episode: 7900, Reward: -13, Eps: 0.04200420042004201\n",
      "Episode: 8000, Reward: -119, Eps: 0.040004000400040006\n",
      "Episode: 8100, Reward: -13, Eps: 0.038003800380038005\n",
      "Episode: 8200, Reward: -13, Eps: 0.036003600360036005\n",
      "Episode: 8300, Reward: -13, Eps: 0.034003400340034004\n",
      "Episode: 8400, Reward: -13, Eps: 0.032003200320032\n",
      "Episode: 8500, Reward: -13, Eps: 0.030003000300030003\n",
      "Episode: 8600, Reward: -13, Eps: 0.028002800280028002\n",
      "Episode: 8700, Reward: -13, Eps: 0.026002600260026\n",
      "Episode: 8800, Reward: -13, Eps: 0.024002400240024\n",
      "Episode: 8900, Reward: -13, Eps: 0.022002200220022004\n",
      "Episode: 9000, Reward: -13, Eps: 0.020002000200020003\n",
      "Episode: 9100, Reward: -13, Eps: 0.018001800180018002\n",
      "Episode: 9200, Reward: -13, Eps: 0.016001600160016\n",
      "Episode: 9300, Reward: -122, Eps: 0.014001400140014001\n",
      "Episode: 9400, Reward: -13, Eps: 0.012001200120012\n",
      "Episode: 9500, Reward: -13, Eps: 0.010001000100010001\n",
      "Episode: 9600, Reward: -13, Eps: 0.008000800080008\n",
      "Episode: 9700, Reward: -13, Eps: 0.006000600060006\n",
      "Episode: 9800, Reward: -13, Eps: 0.004000400040004\n",
      "Episode: 9900, Reward: -13, Eps: 0.002000200020002\n",
      "Episode: 10000, Reward: -13, Eps: 0.0\n",
      "Episode: 100, Reward: -30, Eps: 0.19801980198019803\n",
      "Episode: 200, Reward: -33, Eps: 0.19601960196019602\n",
      "Episode: 300, Reward: -19, Eps: 0.19401940194019401\n",
      "Episode: 400, Reward: -23, Eps: 0.192019201920192\n",
      "Episode: 500, Reward: -26, Eps: 0.19001900190019003\n",
      "Episode: 600, Reward: -18, Eps: 0.18801880188018802\n",
      "Episode: 700, Reward: -17, Eps: 0.186018601860186\n",
      "Episode: 800, Reward: -118, Eps: 0.18401840184018403\n",
      "Episode: 900, Reward: -18, Eps: 0.18201820182018202\n",
      "Episode: 1000, Reward: -25, Eps: 0.18001800180018002\n",
      "Episode: 1100, Reward: -22, Eps: 0.178017801780178\n",
      "Episode: 1200, Reward: -31, Eps: 0.17601760176017603\n",
      "Episode: 1300, Reward: -29, Eps: 0.17401740174017402\n",
      "Episode: 1400, Reward: -25, Eps: 0.172017201720172\n",
      "Episode: 1500, Reward: -20, Eps: 0.17001700170017\n",
      "Episode: 1600, Reward: -21, Eps: 0.16801680168016803\n",
      "Episode: 1700, Reward: -120, Eps: 0.16601660166016602\n",
      "Episode: 1800, Reward: -18, Eps: 0.164016401640164\n",
      "Episode: 1900, Reward: -18, Eps: 0.162016201620162\n",
      "Episode: 2000, Reward: -21, Eps: 0.16001600160016002\n",
      "Episode: 2100, Reward: -17, Eps: 0.15801580158015802\n",
      "Episode: 2200, Reward: -17, Eps: 0.156015601560156\n",
      "Episode: 2300, Reward: -18, Eps: 0.15401540154015403\n",
      "Episode: 2400, Reward: -20, Eps: 0.15201520152015202\n",
      "Episode: 2500, Reward: -18, Eps: 0.15001500150015\n",
      "Episode: 2600, Reward: -17, Eps: 0.148014801480148\n",
      "Episode: 2700, Reward: -21, Eps: 0.14601460146014603\n",
      "Episode: 2800, Reward: -25, Eps: 0.14401440144014402\n",
      "Episode: 2900, Reward: -20, Eps: 0.142014201420142\n",
      "Episode: 3000, Reward: -18, Eps: 0.14001400140014\n",
      "Episode: 3100, Reward: -22, Eps: 0.13801380138013802\n",
      "Episode: 3200, Reward: -17, Eps: 0.13601360136013602\n",
      "Episode: 3300, Reward: -122, Eps: 0.134013401340134\n",
      "Episode: 3400, Reward: -17, Eps: 0.132013201320132\n",
      "Episode: 3500, Reward: -19, Eps: 0.13001300130013002\n",
      "Episode: 3600, Reward: -24, Eps: 0.128012801280128\n",
      "Episode: 3700, Reward: -18, Eps: 0.126012601260126\n",
      "Episode: 3800, Reward: -19, Eps: 0.12401240124012401\n",
      "Episode: 3900, Reward: -23, Eps: 0.12201220122012202\n",
      "Episode: 4000, Reward: -20, Eps: 0.12001200120012001\n",
      "Episode: 4100, Reward: -19, Eps: 0.11801180118011802\n",
      "Episode: 4200, Reward: -17, Eps: 0.11601160116011601\n",
      "Episode: 4300, Reward: -17, Eps: 0.11401140114011402\n",
      "Episode: 4400, Reward: -19, Eps: 0.11201120112011201\n",
      "Episode: 4500, Reward: -19, Eps: 0.11001100110011001\n",
      "Episode: 4600, Reward: -18, Eps: 0.10801080108010801\n",
      "Episode: 4700, Reward: -17, Eps: 0.10601060106010601\n",
      "Episode: 4800, Reward: -20, Eps: 0.104010401040104\n",
      "Episode: 4900, Reward: -18, Eps: 0.10201020102010201\n",
      "Episode: 5000, Reward: -17, Eps: 0.1000100010001\n",
      "Episode: 5100, Reward: -21, Eps: 0.09800980098009801\n",
      "Episode: 5200, Reward: -17, Eps: 0.096009600960096\n",
      "Episode: 5300, Reward: -21, Eps: 0.09400940094009401\n",
      "Episode: 5400, Reward: -17, Eps: 0.09200920092009202\n",
      "Episode: 5500, Reward: -18, Eps: 0.09000900090009001\n",
      "Episode: 5600, Reward: -17, Eps: 0.08800880088008801\n",
      "Episode: 5700, Reward: -19, Eps: 0.086008600860086\n",
      "Episode: 5800, Reward: -19, Eps: 0.08400840084008401\n",
      "Episode: 5900, Reward: -17, Eps: 0.082008200820082\n",
      "Episode: 6000, Reward: -17, Eps: 0.08000800080008001\n",
      "Episode: 6100, Reward: -17, Eps: 0.078007800780078\n",
      "Episode: 6200, Reward: -19, Eps: 0.07600760076007601\n",
      "Episode: 6300, Reward: -17, Eps: 0.074007400740074\n",
      "Episode: 6400, Reward: -17, Eps: 0.07200720072007201\n",
      "Episode: 6500, Reward: -18, Eps: 0.07000700070007\n",
      "Episode: 6600, Reward: -20, Eps: 0.06800680068006801\n",
      "Episode: 6700, Reward: -21, Eps: 0.066006600660066\n",
      "Episode: 6800, Reward: -21, Eps: 0.064006400640064\n",
      "Episode: 6900, Reward: -17, Eps: 0.062006200620062006\n",
      "Episode: 7000, Reward: -17, Eps: 0.060006000600060005\n",
      "Episode: 7100, Reward: -17, Eps: 0.058005800580058005\n",
      "Episode: 7200, Reward: -19, Eps: 0.056005600560056004\n",
      "Episode: 7300, Reward: -17, Eps: 0.054005400540054004\n",
      "Episode: 7400, Reward: -17, Eps: 0.052005200520052\n",
      "Episode: 7500, Reward: -17, Eps: 0.05000500050005\n",
      "Episode: 7600, Reward: -19, Eps: 0.048004800480048\n",
      "Episode: 7700, Reward: -18, Eps: 0.04600460046004601\n",
      "Episode: 7800, Reward: -17, Eps: 0.04400440044004401\n",
      "Episode: 7900, Reward: -19, Eps: 0.04200420042004201\n",
      "Episode: 8000, Reward: -17, Eps: 0.040004000400040006\n",
      "Episode: 8100, Reward: -17, Eps: 0.038003800380038005\n",
      "Episode: 8200, Reward: -17, Eps: 0.036003600360036005\n",
      "Episode: 8300, Reward: -18, Eps: 0.034003400340034004\n",
      "Episode: 8400, Reward: -17, Eps: 0.032003200320032\n",
      "Episode: 8500, Reward: -17, Eps: 0.030003000300030003\n",
      "Episode: 8600, Reward: -17, Eps: 0.028002800280028002\n",
      "Episode: 8700, Reward: -17, Eps: 0.026002600260026\n",
      "Episode: 8800, Reward: -18, Eps: 0.024002400240024\n",
      "Episode: 8900, Reward: -19, Eps: 0.022002200220022004\n",
      "Episode: 9000, Reward: -17, Eps: 0.020002000200020003\n",
      "Episode: 9100, Reward: -17, Eps: 0.018001800180018002\n",
      "Episode: 9200, Reward: -17, Eps: 0.016001600160016\n",
      "Episode: 9300, Reward: -17, Eps: 0.014001400140014001\n",
      "Episode: 9400, Reward: -17, Eps: 0.012001200120012\n",
      "Episode: 9500, Reward: -17, Eps: 0.010001000100010001\n",
      "Episode: 9600, Reward: -17, Eps: 0.008000800080008\n",
      "Episode: 9700, Reward: -17, Eps: 0.006000600060006\n",
      "Episode: 9800, Reward: -17, Eps: 0.004000400040004\n",
      "Episode: 9900, Reward: -17, Eps: 0.002000200020002\n",
      "Episode: 10000, Reward: -17, Eps: 0.0\n"
     ]
    }
   ],
   "source": [
    "Q = learn('Q')\n",
    "SARSA = learn('SARSA')\n",
    "# сохранение\n",
    "env = gym.make('CliffWalking-v0')\n",
    "states = env.observation_space.n\n",
    "actions = env.action_space.n\n",
    "Q_dict = {}\n",
    "pi_Q = {}\n",
    "SARSA_dict = {}\n",
    "pi_SARSA = {}\n",
    "for s in range(states):\n",
    "    Q_dict[s] = {}\n",
    "    SARSA_dict[s] = {}\n",
    "\n",
    "    # Задайте жадную стратегию !!!!!!!!\n",
    "    # TODO\n",
    "    pi_SARSA[s] = int(np.argmax(SARSA[s]))\n",
    "    pi_Q[s] = int(np.argmax(Q[s]))\n",
    "\n",
    "    for a in range(actions):\n",
    "        Q_dict[s][a] = Q[s, a]\n",
    "        SARSA_dict[s][a] = SARSA[s, a]\n",
    "\n",
    "with open('submit.json', \"w\") as f:\n",
    "    json.dump([Q_dict, pi_Q, SARSA_dict, pi_SARSA], f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}